{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класифікатор спаму: підготовка корпусу SpamAssassin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відкрита база листів [SpamAssassin Public Corpus](https://spamassassin.apache.org/old/publiccorpus/) складається з кількох наборів текстових файлів, де кожен файл — дамп email-протоколу для одного листа. Тому, початкова мета — перетворити ці дані в формат, легший для моделювання.\n",
    "\n",
    "Ми виконаємо такі операції:\n",
    "\n",
    "1. Залишимо тільки тіла листів (email body), і вилучимо всі службові заголовки email-протоколів.\n",
    "2. Приберемо всі шумові слова (stop words) — артиклі, прийменники тощо.\n",
    "3. Замінимо всі URL, числа та email-адреси на службові слова `HTTPADDRESS`, `NUMBER`, `EMAILADDRESS`.\n",
    "4. Всі слова, що залишилися, нормалізуємо за допомогою [стемінгу (stemming)](https://en.wikipedia.org/wiki/Stemming).\n",
    "5. Збережемо слова кожного листа у вигляді одного великого JSON-масиву."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm as progressbar\n",
    "# import tqdm import tqdm.notebook as progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завантажимо необхідні мовні моделі та словники:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розділення листів на слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відокремлюємо тіла листів на ділимо їх на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_email_filename = re.compile(r\"[0-9a-f]{5}\\.[0-9a-f]{32}\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_url = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_email = re.compile(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_number = re.compile(r\"\\b\\d+\\b\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_hash = re.compile(r\"[0-9a-f]{32}\", flags=re.MULTILINE | re.UNICODE)\n",
    "re_non_word = re.compile(r\"\\W+\", flags=re.MULTILINE | re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_body_to_words(body, stopwords, stemmer):\n",
    "    # Convert to lowercase.\n",
    "    body = body.lower()\n",
    "\n",
    "    # Replace URLs.\n",
    "    body = re_url.sub(\"HTTPADDRESS\", body)\n",
    "\n",
    "    # Replace email addresses.\n",
    "    body = re_email.sub(\"EMAILADDRESS\", body)\n",
    "\n",
    "    # Replace hashes.\n",
    "    body = re_hash.sub(\"HASH\", body)\n",
    "    \n",
    "    # Replace numbers.\n",
    "    body = re_number.sub(\"NUMBER\", body)\n",
    "\n",
    "    # Remove non-word characters (punctuation, spacers, etc.)\n",
    "    body = re_non_word.sub(\" \", body)\n",
    "\n",
    "    # Remove trailing spaces.\n",
    "    body = body.strip()\n",
    "\n",
    "    # Remove stopwords and stem the remaining words.\n",
    "    words = [stemmer.stem(word) for word in body.split() if word not in stopwords]\n",
    "    \n",
    "    # Remove technical-looking words (base64, underscores, etc.) words.\n",
    "    words = [word for word in words if len(word) <= 20 and not \"_\" in word and not any(ch.isdigit() for ch in word)]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_emails_in_folders(folders, filename_regex, stopwords, stemmer):\n",
    "    emails = []\n",
    "    \n",
    "    # Iterate through every folder in our dataset.\n",
    "    for folder in progressbar(folders, desc=\"Folders\"):\n",
    "        email_filenames = [\n",
    "            filename\n",
    "            for filename in sorted(os.listdir(folder))\n",
    "            if filename_regex.match(os.path.basename(filename))\n",
    "        ]\n",
    "\n",
    "        # Each email is kept as a single file, iterate through them.\n",
    "        for file in progressbar(email_filenames, desc=os.path.basename(folder)):\n",
    "            email_path = os.path.join(folder, file)\n",
    "            \n",
    "            with open(email_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as email_file:\n",
    "                msg = email.message_from_file(email_file)\n",
    "                if not msg.is_multipart():\n",
    "                    body = msg.get_payload()\n",
    "                    words = email_body_to_words(body, stopwords, stemmer)\n",
    "                    emails.append(words)\n",
    "    \n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d893c99490460596fcddfa72ff2773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folders:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4d59192f8541ce8d4fe72198ba1691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "easy_ham:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0257482d71be4f708ee7c0971b80fe5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "easy_ham_2:   0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3d147679544f6a93f6dc85281f998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hard_ham:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emails_tokenized_ham = tokenize_emails_in_folders(\n",
    "    [\n",
    "        r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\easy_ham\",\n",
    "        r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\easy_ham_2\",\n",
    "        r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\hard_ham\"\n",
    "    ],\n",
    "    re_email_filename,\n",
    "    stopwords,\n",
    "    stemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec10097dc0e44a0a64da9e2a9fd6b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folders:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98178a58094b447bb2a141aaa8ccc440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spam:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ef9ad4d2fb4442843ae0f643eddebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spam_2:   0%|          | 0/1397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emails_tokenized_spam = tokenize_emails_in_folders(\n",
    "    [\n",
    "        r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\spam\",\n",
    "        r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\spam_2\"\n",
    "    ],\n",
    "    re_email_filename,\n",
    "    stopwords,\n",
    "    stemmer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Ham emails:   3952\n",
      "# Spam emails:  1591\n"
     ]
    }
   ],
   "source": [
    "print(\"# Ham emails:  \", len(emails_tokenized_ham))\n",
    "print(\"# Spam emails: \", len(emails_tokenized_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json_file(obj, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json_file(emails_tokenized_ham, r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\emails-tokenized-ham.json\")\n",
    "save_as_json_file(emails_tokenized_spam, r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\emails-tokenized-spam.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Побудова словника"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша мета — присвоїти порядковий номер кожному унікальному слову в базі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримуємо множину всіх унікальних слів:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set(itertools.chain(*emails_tokenized_ham)).union(set(itertools.chain(*emails_tokenized_spam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формуємо з неї словник: _cлово_ -> _номер_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    word: index\n",
    "    for index, word in enumerate(sorted(list(vocab_set)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 34147\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary length:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевіримо наш словних на кількох випадкових словах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy        3957\n",
      "watch      32227\n",
      "discount   7621\n"
     ]
    }
   ],
   "source": [
    "for word in [\"buy\", \"watch\", \"discount\"]:\n",
    "    print(word.ljust(10), vocab[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збережемо словник у файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json_file(vocab, r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Огляд результатів"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевіримо, як наш метод очищення листів працює на прикладі довільного листа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_email = email.message_from_string(open(\\\n",
    "                        r\"C:\\Users\\user\\Jupyter\\gl-ml-basecamp\\HW-3\\data\\easy_ham\\01306.01273f7d32eaabde7b20f220e13eb927\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "\n",
      "Has anyone made a working source RPM for dvd::rip for Red Hat 8.0?\n",
      "Matthias has a spec file on the site for 0.46, and there are a couple of\n",
      "spec files lying around on the dvd::rip website, including one I patched\n",
      "a while ago, but it appears that the Makefile automatically generated is\n",
      "trying to install the Perl libraries into the system's, and also at the\n",
      "moment dvd::rip needs to be called with PERLIO=stdio as it seems to not\n",
      "work with PerlIO on RH8's Perl.\n",
      "\n",
      "Not too sure what the cleanest way to fix this is - anyone working on\n",
      "this?\n",
      "\n",
      "Thanks,\n",
      "\n",
      "-- \n",
      "MichГЁl Alexandre Salim\n",
      "Web:\t\thttp://salimma.freeshell.org\n",
      "GPG/PGP key:\thttp://salimma.freeshell.org/files/crypto/publickey.asc\n",
      "\n",
      "__________________________________________________\n",
      "Do You Yahoo!?\n",
      "Everything you'll ever need on one web page\n",
      "from News and Sport to Email and Music Charts\n",
      "http://uk.my.yahoo.com\n",
      "\n",
      "_______________________________________________\n",
      "RPM-List mailing list <RPM-List@freshrpms.net>\n",
      "http://lists.freshrpms.net/mailman/listinfo/rpm-list\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_body = sample_email.get_payload()\n",
    "print(sample_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'anyon', 'made', 'work', 'sourc', 'rpm', 'dvd', 'rip', 'red', 'hat', 'number', 'number', 'matthia', 'spec', 'file', 'site', 'number', 'number', 'coupl', 'spec', 'file', 'lie', 'around', 'dvd', 'rip', 'websit', 'includ', 'one', 'patch', 'ago', 'appear', 'makefil', 'automat', 'generat', 'tri', 'instal', 'perl', 'librari', 'system', 'also', 'moment', 'dvd', 'rip', 'need', 'call', 'perlio', 'stdio', 'seem', 'work', 'perlio', 'perl', 'sure', 'cleanest', 'way', 'fix', 'anyon', 'work', 'thank', 'michгёl', 'alexandr', 'salim', 'web', 'httpaddress', 'gpg', 'pgp', 'key', 'httpaddress', 'yahoo', 'everyth', 'ever', 'need', 'one', 'web', 'page', 'news', 'sport', 'email', 'music', 'chart', 'httpaddress', 'rpm', 'list', 'mail', 'list', 'emailaddress', 'httpaddress']\n"
     ]
    }
   ],
   "source": [
    "print(email_body_to_words(sample_body, stopwords, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
